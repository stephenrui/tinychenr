from read_data import *
from keras.layers import*
from keras.models import *
from keras import Sequential
from keras.optimizers import *
from keras.callbacks import *
import pandas as pd
from read_txt import loadtimes

visit_size=25
n_chanels = 3
c=time.clock()
train_visit = 'data\\train\\'
test_visit = 'data\\test\\'
str1='data\\train_image\\001\*.jpg'
str2='data\\train_image\\002\*.jpg'
str3='data\\train_image\\003\*.jpg'
str4='data\\train_image\\004\*.jpg'
str5='data\\train_image\\005\*.jpg'
str6='data\\train_image\\006\*.jpg'
str7='data\\train_image\\007\*.jpg'
str8='data\\train_image\\008\*.jpg'
str9='data\\train_image\\009\*.jpg'
str_tr1='data\\test_load\\train\\1\\*.jpg'
str_tr2='data\\test_load\\train\\2\\*.jpg'
str_tr3='data\\test_load\\train\\3\\*.jpg'
str_tr4='data\\test_load\\train\\4\\*.jpg'
str_tr5='data\\test_load\\train\\5\\*.jpg'
str_tr6='data\\test_load\\train\\6\\*.jpg'
str_tr7='data\\test_load\\train\\7\\*.jpg'
str_tr8='data\\test_load\\train\\8\\*.jpg'
str_tr9='data\\test_load\\train\\9\\*.jpg'
str_te='data\\test_load\\test\\*.jpg'
trfiles=[str_tr1,str_tr2]#,str_tr3,str_tr4,str_tr5,str_tr6,str_tr7,str_tr8,str_tr9]#[str1,str2,str3,str4,str5,str6,str7,str8,str9]
n_speces = len(trfiles)
trsample,_ = concate_data(trfiles,False)
t0 = time.clock()
for i in range(len(trsample)):
    trsample[i][2] = train_visit+trsample[i][2][:10]+'.txt'
    trsample[i][2],_ = loadtimes(trsample[i][2])
    if i%100==0:
        pro = 1.0*i/len(trsample)
        a=time.asctime(time.localtime(time.time()))
        print(a,'---------Reading training data----------%f%%  %d/%d'%(100*pro,i,len(trsample)))
df1=pd.DataFrame(trsample)


str11='data\\test_image\\*.jpg'
'''str22='data\\test_image\\002\*.jpg'
str33='data\\test_image\\003\*.jpg'
str44='data\\test_image\\004\*.jpg'
str55='data\\test_image\\005\*.jpg'''
tefiles=[str_te]#[str11]
tesample,org = concate_data(tefiles,False)
for i in range(len(tesample)):
    tesample[i][2] = test_visit+tesample[i][2][:6]+'.txt'
    tesample[i][2],_ = loadtimes(tesample[i][2])
    if i%100==0:
        pro = 1.0*i/len(tesample)
        a=time.asctime(time.localtime(time.time()))
        print(a,'---------Reading testing data----------%f%%  %d/%d'%(100*pro,i,len(tesample)))
df2=pd.DataFrame(tesample)
print('load image time: ',time.clock()-c)
Train_data = df1.sample(n=len(trsample))
Train_xImage = Train_data[0].tolist()
Train_xImage = np.reshape(Train_xImage,(-1,img_size,img_size,n_chanels))
Train_xVisit = np.array(Train_data[2].tolist())
Train_xVisit = np.reshape(Train_xVisit,(-1,visit_size))
Train_y = Train_data[1].tolist()
Train_y = np.reshape(Train_y,(-1,n_speces))
Test_data = df2.sample(n=len(tesample))
Test_xImage = Test_data[0].tolist()
Test_xImage = np.reshape(Test_xImage,(-1,img_size,img_size,n_chanels))
Test_xVisit = np.array(Test_data[2].tolist())
Test_xVisit = np.reshape(Test_xVisit,(-1,visit_size))
print(Train_xImage[0])
def ResBlock3(map,chanels,conv1_stride=1):
    sizes = map.shape[2]
    map= BatchNormalization(axis=3)(map)
    conv1 = Conv2D(chanels,(1,1),strides=(conv1_stride,conv1_stride),padding='same',activation='relu')(map)
    conv1= BatchNormalization(axis=3)(conv1)
    conv2 = Conv2D(chanels,(3,3),padding='same',activation='relu')(conv1)
    conv2= BatchNormalization(axis=3)(conv2)
    conv2 = Conv2D(chanels*4,(1,1),padding='same',activation='relu')(conv2)
    conv_pad = Conv2D(chanels*4,(1,1),padding='same')(conv1)
    conv2 = add([conv2,conv_pad])
    print(conv2.shape)
    return conv2

Image_Input = Input(shape=(img_size,img_size,n_chanels),name='Image_Input')

conv1 = Conv2D(64,(5,5),padding='same',activation='relu')(Image_Input)
conv1 = MaxPooling2D(pool_size=(3,3),strides=(2,2))(conv1)
conv1 = ResBlock3(conv1,chanels=64,conv1_stride=2)
conv1 = ResBlock3(conv1,chanels=64,conv1_stride=1)
conv1 = ResBlock3(conv1,chanels=64,conv1_stride=1)
conv2 = ResBlock3(conv1,chanels=128,conv1_stride=2)
conv2 = ResBlock3(conv2,chanels=128,conv1_stride=1)
conv2 = ResBlock3(conv2,chanels=128,conv1_stride=1)
conv2 = ResBlock3(conv2,chanels=128,conv1_stride=1)
conv3 = ResBlock3(conv2,chanels=256,conv1_stride=2)
conv3 = ResBlock3(conv3,chanels=256,conv1_stride=1)
conv3 = ResBlock3(conv3,chanels=256,conv1_stride=1)
conv3 = ResBlock3(conv3,chanels=256,conv1_stride=1)
conv3 = ResBlock3(conv3,chanels=256,conv1_stride=1)
conv3 = ResBlock3(conv3,chanels=256,conv1_stride=1)
conv4 = ResBlock3(conv3,chanels=512,conv1_stride=2)
conv4 = ResBlock3(conv4,chanels=512,conv1_stride=1)
conv4 = ResBlock3(conv4,chanels=512,conv1_stride=1)
average_pool =AveragePooling2D((int(conv4.shape[1]),int(conv4.shape[2])))(conv4)
fc1 = Flatten()(average_pool)
fc1 = BatchNormalization()(fc1)
fc1 = Dense(1000,activation='relu')(fc1)
fc1 = Dropout(0.25)(fc1)
fc1 = Dense(n_speces,activation='softmax')(fc1)

adam = Adam(lr=0.001)
reduce_lr = ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.8,min_lr=1e-4)
checkpoint = ModelCheckpoint('model\\image\\ResNet_keras.h5',
                             monitor='val_loss',mode='min',save_best_only=True)

model = Model(inputs = Image_Input,outputs = fc1)
model.summary()
model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])
print("Training---------")
model.fit(x=Train_xImage,y=Train_y,batch_size=64,epochs=50,validation_split=0.1)
